# 6.1 Distributed id generator

Sometimes we need to be able to generate an id that is similar to the MySQL auto-increment ID and that doesn't repeat. To support high concurrency scenarios in the business. More typical, when e-commerce promotions, a large number of orders will flood into the system in a short period of time, such as 10w+ per second. When the star is derailed, there will be a lot of enthusiastic fans who will send a microblog to express their wishes, and will also generate a lot of news in a short time.

Before inserting the database, we need to give these messages, the order an ID first, and then insert it into our database. The requirement for this id is that it can have some time information, so that even if our back-end system sub-databases the message, it can sort the messages in chronological order.

Twitter's snowflake algorithm is a typical solution in this scenario. Let's first take a look at what snowflake is all about, see *Figure 6-1*:

![snowflake](../images/ch6-snowflake.png)

*Figure 6-1 Bit Distribution in snowflake*

First determine that our value is 64-bit, int64 type, divided into four parts, without the first bit of the beginning, because this bit is a sign bit. Use 41 bits to indicate the timestamp when the request was received, in milliseconds, then five digits to represent the id of the data center, then five digits to represent the instance id of the machine, and finally the 12-bit loop increment id (to 1111) , 1111, 1111 will return to 0).

Such a mechanism can support us to generate `2 ^ 12 = 4096` messages in the same millisecond on the same machine. A total of 4.096 million messages in one second. It is completely sufficient from the value range.

The data center plus instance id has a total of 10 digits, which can support 32 machines per data center and 1024 instances in all data centers.

Indicates the 41 bits of `timestamp`, which can support us for 69 years. Of course, our time millisecond count won't really start from 1970, so our system can't be used until `2039/9/7 23:47:35`, so the `timestamp` here is only relative to some The increment of time, for example, our system is online 2018-08-01, then we can treat this timestamp as an offset from `2018-08-01 00:00:00.000`.

## 6.1.1 worker_id allocation

Among the four fields `timestamp`, `datacenter_id`, `worker_id` and `sequence_id`, `timestamp` and `sequence_id` are generated by the program at runtime. But `datacenter_id` and `worker_id` need to be available during the deployment phase, and once the program is started, it can't be changed. (Think, if you can change it at will, you may be inadvertently modified, resulting in the final generated id. conflict).

Generally, the machines in different data centers will provide corresponding APIs for obtaining data center ids, so `datacenter_id` can be easily obtained during the deployment phase. And worker_id is an id that we logically assign to the machine. What should we do? The simpler idea is supported by tools that provide this self-increasing id feature, such as MySQL:

```shell
Mysql> insert into a (ip) values("10.1.2.101");
Query OK, 1 row affected (0.00 sec)

Mysql> select last_insert_id();
+------------------+
| last_insert_id() |
+------------------+
| 2 |
+------------------+
1 row in set (0.00 sec)
```

After getting `worker_id` from MySQL, we will persist this `worker_id` directly to the local to avoid getting a new `worker_id` every time you go online. Let the single instance's `worker_id` be left unchanged.

Of course, using MySQL is equivalent to adding an external dependency to our simple id generation service. The more dependencies, the worse the serviceability of our services.

Considering that even if there is a single id generation service instance hanging in the cluster, it is a part of the id lost for a while, so we can also be more violent, write `worker_id` directly in the worker configuration, when deployed, by deployment The script completes the replacement of the `worker_id` field.

## 6.1.2 Open source instance

### 6.1.2.1 Standard snowflake implementation

`github.com/bwmarrin/snowflake` is a fairly lightweight implementation of Snowflake's Go. The definition of the use of each document is shown in Figure 6-2*.

![ch6-snowflake-easy](../images/ch6-snowflake-easy.png)

*Figure 6-2 snowflake library*

It is exactly the same as the standard snowflake. It is relatively simple to use:

```go
Package main

Import (
"fmt"
"os"

"github.com/bwmarrin/snowflake"
)

Func main() {
n, err := snowflake.NewNode(1)
If err != nil {
Println(err)
os.Exit(1)
}

For i := 0; i < 3; i++ {
Id := n.Generate()
fmt.Println("id", id)
fmt.Println(
"node: ", id.Node(),
"step: ", id.Step(),
"time: ", id.Time(),
"\n",
)
}
}
```

Of course, this library also left us with a customized backend, which reserved some customizable fields:

```go
// Epoch is set to the twitter snowflake epoch of Nov 04 2010 01:42:54 UTC
// You may customize this to set a different epoch for your application.
Epoch int64 = 1288834974657

// Number of bits to use for Node
// Remember, you have a total 22 bits to share between Node/Step
NodeBits uint8 = 10

// Number of bits to use for Step
// Remember, you have a total 22 bits to share between Node/Step
StepBits uint8 = 12
```

`Epoch` is the start time at the beginning of this section, `NodeBits` refers to the bit length of the machine number, and `StepBits` refers to the bit length of the self-increasing sequence.

### 6.1.2.2 sonyflake

Sonyflake is an open source project of Sony. The basic idea is similar to snowflake, but the bit allocation is slightly different. See *Figure 6-3*:

![sonyflake](../images/ch6-snoyflake.png)

*Figure 6-3 sonyflake*

The time here only uses 39 bits, but the unit of time becomes 10ms, so theoretically it is longer than the time indicated by 41 bits (174 years).

`Sequence ID` is consistent with the previous definition, `Machine ID` is actually the node id. The difference between `sonyflake` is its configuration parameters during the startup phase:

```go
Func NewSonyflake(st Settings) *Sonyflake
```

The `Settings` data structure is as follows:

```go
Type Settings struct {
StartTime time.Time
MachineID func() (uint16, error)
CheckMachineID func(uint16) bool
}
```

The `StartTime` option is similar to our previous `Epoch`. If not set, the default is to start with `2014-09-01 00:00:00 +0000 UTC`.

`MachineID` can be a user-defined function. If the user does not define it, the lower 16 bits of the native IP will be used as the `machine id` by default.

`CheckMachineID` is a function provided by the user to check if `MachineID` conflicts. The design here is quite clever. If there is another centralized storage and support for checking the duplicate storage, then we can customize the logic to check if the `MachineID` conflicts according to our own ideas. If the company has a ready-made Redis cluster, then we can easily check for conflicts with Redis' collection types.

```shell
Redis 127.0.0.1:6379> SADD base64_encoding_of_last16bits MzI0Mgo=
(integer) 1
Redis 127.0.0.1:6379> SADD base64_encoding_of_last16bits MzI0Mgo=
(integer) 0
```

It is also relatively simple to use, and some logic-simple functions are omitted:

```go
Package main

Import (
"fmt"
"os"
"time"

"github.com/sony/sonyflake"
)

Func getMachineID() (uint16, error) {
Var machineID uint16
Var err error
machineID = readMachineIDFromLocalFile()
If machineID == 0 {
machineID, err = generateMachineID()
If err != nil {
Return 0, err
}
}

Return machineID, nil
}

Func checkMachineID(machineID uint16) bool {
saddResult, err := saddMachineIDToRedisSet()
If err != nil || saddResult == 0 {
Return true
}

Err := saveMachineIDToLocalFile(machineID)
If err != nil {
Return true
}

Return false
}

Func main() {
t, _ := time.Parse("2006-01-02", "2018-01-01")
Settings := sonyflake.Settings{
StartTime: t,
MachineID: getMachineID,
CheckMachineID: checkMachineID,
}

Sf := sonyflake.NewSonyflake(settings)
Id, err := sf.NextID()
If err != nil {
fmt.Println(err)
os.Exit(1)
}

fmt.Println(id)
}
```